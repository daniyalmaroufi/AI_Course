{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![University of Tehran](./img/UT.png)\n",
    "#   <font color='red'><center>AI CA 3<center></font> \n",
    "## <center>Dr. Fadaei<center>\n",
    "### <center>Daniyal Maroufi<center>\n",
    "### <center>810098039<center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim\n",
    "\n",
    "This assignment aims to use Naive Bayes networks to build a classifier model to predict the category of an article from Digikala using its excerpt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from __future__ import unicode_literals\n",
    "from hazm import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "First, we read the training and test data from csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>فیلم‌های در حال اکران؛ موزیکال شاد خاله قورباغ...</td>\n",
       "      <td>هنر و سینما</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>پنج فیلمسازی که کوئنتین تارانتینو را عاشق سینم...</td>\n",
       "      <td>هنر و سینما</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>جانی آیو از اپل رفت جانی آیو دیگر نیازی به معر...</td>\n",
       "      <td>علم و تکنولوژی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>احتمال عدم پشتیبانی iOS ۱۳ از آیفون ۵ اس، SE و...</td>\n",
       "      <td>علم و تکنولوژی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>دزدان مغازه نماینده ژاپن در اسکار ۲۰۱۹ شد فیلم...</td>\n",
       "      <td>هنر و سینما</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5195</th>\n",
       "      <td>امپراطوری اپ (فصل اول/بخش دوم) فصل اول – بخش د...</td>\n",
       "      <td>سلامت و زیبایی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5196</th>\n",
       "      <td>عدم ارتباطات اثربخش و تعارض در محیط کار وجود س...</td>\n",
       "      <td>سلامت و زیبایی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5197</th>\n",
       "      <td>اپل در سال ۲۰۲۰ چهار آیفون معرفی خواهد کرد! طب...</td>\n",
       "      <td>علم و تکنولوژی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5198</th>\n",
       "      <td>مارتینز: بلژیک باید مقابل فرانسه بدون ترس بازی...</td>\n",
       "      <td>سلامت و زیبایی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5199</th>\n",
       "      <td>شیائومی تاریخ عرضه‌ی گوشی گیمینگ Black Shark ۲...</td>\n",
       "      <td>علم و تکنولوژی</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content           label\n",
       "0     فیلم‌های در حال اکران؛ موزیکال شاد خاله قورباغ...     هنر و سینما\n",
       "1     پنج فیلمسازی که کوئنتین تارانتینو را عاشق سینم...     هنر و سینما\n",
       "2     جانی آیو از اپل رفت جانی آیو دیگر نیازی به معر...  علم و تکنولوژی\n",
       "3     احتمال عدم پشتیبانی iOS ۱۳ از آیفون ۵ اس، SE و...  علم و تکنولوژی\n",
       "4     دزدان مغازه نماینده ژاپن در اسکار ۲۰۱۹ شد فیلم...     هنر و سینما\n",
       "...                                                 ...             ...\n",
       "5195  امپراطوری اپ (فصل اول/بخش دوم) فصل اول – بخش د...  سلامت و زیبایی\n",
       "5196  عدم ارتباطات اثربخش و تعارض در محیط کار وجود س...  سلامت و زیبایی\n",
       "5197  اپل در سال ۲۰۲۰ چهار آیفون معرفی خواهد کرد! طب...  علم و تکنولوژی\n",
       "5198  مارتینز: بلژیک باید مقابل فرانسه بدون ترس بازی...  سلامت و زیبایی\n",
       "5199  شیائومی تاریخ عرضه‌ی گوشی گیمینگ Black Shark ۲...  علم و تکنولوژی\n",
       "\n",
       "[5200 rows x 2 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df=pd.read_csv('./Data/train.csv')\n",
    "test_df=pd.read_csv('./Data/test.csv')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "content    1\n",
       "label      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "content    0\n",
       "label      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because only one sample is null, we simply remove it from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=train_df.dropna(how='any',axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1 - Data Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Natural Language Processing, it is agreed that using the root of the words is better for classification accuracy. The prefixes and postfixes of the words are not that necessary to be in sequence, and sometimes they may even inversely affect the accuracy because all forms of a word have the same meaning. For this purpose, there are two methods, Stemming and Lemmatization. Stemming removes the most common prefixes and postfixes of a word to find the root, while, Lemmatization uses an entire dictionary and finds the actual root of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    normalizer = Normalizer()\n",
    "    lemmatizer = Lemmatizer()\n",
    "    df['content']=df['content'].apply(lambda x: normalizer.normalize(x))\n",
    "    df['content']=df['content'].apply(lambda x: word_tokenize(re.sub(r'[^\\w\\s]', '', x)))\n",
    "    stp_words=set(stopwords_list())\n",
    "    df['content']=df['content'].apply(lambda x: [lemmatizer.lemmatize(a) for a in x if a not in stp_words])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=clean_data(train_df)\n",
    "test_df=clean_data(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[فیلم, اکران, موزیکال, شاد, خاله, قورباغه, بزر...</td>\n",
       "      <td>هنر و سینما</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[فیلمسازی, کوئنتین, تارانتینو, عاشق, سینما, کم...</td>\n",
       "      <td>هنر و سینما</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[جان, آیو, اپل, جان, آیو, نیاز, معرف, تقریبا, ...</td>\n",
       "      <td>علم و تکنولوژی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[احتمال, پشتیبان, iOS, ۱۳, آیفون, ۵, اس, SE, آ...</td>\n",
       "      <td>علم و تکنولوژی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[دزد, مغازه, نماینده, ژاپن, اسکار, ۲۰۱۹, فیلم,...</td>\n",
       "      <td>هنر و سینما</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5195</th>\n",
       "      <td>[امپراطوری, اپ, فصل, اولبخش, فصل, دوماپ, گنجین...</td>\n",
       "      <td>سلامت و زیبایی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5196</th>\n",
       "      <td>[ارتباطات, اثربخش, تعارض, محیط, کار, سازمان, و...</td>\n",
       "      <td>سلامت و زیبایی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5197</th>\n",
       "      <td>[اپل, سال, ۲۰۲۰, آیفون, معرف, گزارش, JPMorgan,...</td>\n",
       "      <td>علم و تکنولوژی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5198</th>\n",
       "      <td>[مارتینز, بلژیک, مقابل, فرانسه, ترس, بازی, سرم...</td>\n",
       "      <td>سلامت و زیبایی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5199</th>\n",
       "      <td>[شیائومی, تاریخ, عرضه, گوش, گیمینگ, Black, Sha...</td>\n",
       "      <td>علم و تکنولوژی</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5199 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content           label\n",
       "0     [فیلم, اکران, موزیکال, شاد, خاله, قورباغه, بزر...     هنر و سینما\n",
       "1     [فیلمسازی, کوئنتین, تارانتینو, عاشق, سینما, کم...     هنر و سینما\n",
       "2     [جان, آیو, اپل, جان, آیو, نیاز, معرف, تقریبا, ...  علم و تکنولوژی\n",
       "3     [احتمال, پشتیبان, iOS, ۱۳, آیفون, ۵, اس, SE, آ...  علم و تکنولوژی\n",
       "4     [دزد, مغازه, نماینده, ژاپن, اسکار, ۲۰۱۹, فیلم,...     هنر و سینما\n",
       "...                                                 ...             ...\n",
       "5195  [امپراطوری, اپ, فصل, اولبخش, فصل, دوماپ, گنجین...  سلامت و زیبایی\n",
       "5196  [ارتباطات, اثربخش, تعارض, محیط, کار, سازمان, و...  سلامت و زیبایی\n",
       "5197  [اپل, سال, ۲۰۲۰, آیفون, معرف, گزارش, JPMorgan,...  علم و تکنولوژی\n",
       "5198  [مارتینز, بلژیک, مقابل, فرانسه, ترس, بازی, سرم...  سلامت و زیبایی\n",
       "5199  [شیائومی, تاریخ, عرضه, گوش, گیمینگ, Black, Sha...  علم و تکنولوژی\n",
       "\n",
       "[5199 rows x 2 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2 - Problem Procedure\n",
    "\n",
    "In this assignment, we use the Bag of Words strategy. In this strategy, the position of the words in the sentence is not considered, and only the existence of the words is important. This assumption is not the best one as the order of the words in the sentence is essential too, but on our data, it is good enough to get good accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic formula of the Naive Bayes is shown as bellow:\n",
    "\n",
    "![Naive Bayes](./img/NaiveBayes.jpg)\n",
    "\n",
    "where *evidence, likelihood, prior, posterior probabilities, and predictor prior probability* in our problem are:\n",
    "\n",
    "- The **evidence**(x) is the text input to the model, and the query is the category of the text\n",
    "- The **posterior probability** is the probability of category(c) concerning given evidence(x). \n",
    "- The **likelihood** is the reverse of Posterior probability, the probability of the evidence(x) in category(c).\n",
    "- The **prior probability** is category(c) probability among all categories.\n",
    "- The **predictor prior probability** is the probability of the evidence(x) in a general text.\n",
    "\n",
    "As predictor prior probability is the same for all classes, we only have to compare the nominator, and we can ignore the denominator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping Categories to Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert categorical columns to numerical, we simply use map() pandas method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  -->  هنر و سینما\n",
      "1  -->  علم و تکنولوژی\n",
      "2  -->  سلامت و زیبایی\n",
      "3  -->  بازی ویدیویی\n"
     ]
    }
   ],
   "source": [
    "cats=defaultdict(None)\n",
    "for i, cat in enumerate(train_df['label'].unique()):\n",
    "    cats[cat]=i\n",
    "    print(i,' --> ',cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['label']=train_df['label'].map(cats)\n",
    "test_df['label']=test_df['label'].map(cats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing Train Data to Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_classes=[]\n",
    "for i in range(len(cats)):\n",
    "    train_df_classes.append(train_df.loc[train_df['label'].isin([0])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the Likelihood of the Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_liklihood(class_df):\n",
    "    words_prob=defaultdict(lambda: 1)\n",
    "    for _,row in class_df.iterrows():\n",
    "        for j in range(len(row['content'])):\n",
    "            word=row['content'][j]\n",
    "            words_prob[word]+=1\n",
    "    num_all_words_class=sum(words_prob.values())\n",
    "    for word in words_prob:\n",
    "        words_prob[word]/=num_all_words_class\n",
    "    return words_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_words_prob=[]\n",
    "for i in range(len(cats)):\n",
    "    class_words_prob.append(calc_liklihood(train_df_classes[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this classifier, we ignore the unseen words in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_class_prob_1(test_sample, words_prob):\n",
    "    prob=math.log(1/len(cats))\n",
    "    test_words_prob=[]\n",
    "    for word in test_sample:\n",
    "        if word in words_prob:\n",
    "            test_words_prob.append(math.log(words_prob[word]))\n",
    "    prob = prob + sum(test_words_prob)\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_1_predict(test_sample, class_words_prob):\n",
    "    category_chance=[]\n",
    "    for i in range(len(cats)):\n",
    "        class_prob = calc_class_prob_1(test_sample,class_words_prob[i])\n",
    "        category_chance.append(class_prob)\n",
    "    return category_chance.index(max(category_chance))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_1_evaluate(class_words_prob):\n",
    "    predictions = []\n",
    "    correct_labels=0\n",
    "    for _,row in test_df.iterrows():\n",
    "        prediction=classifier_1_predict(row['content'],class_words_prob)\n",
    "        predictions.append(prediction)\n",
    "        if prediction==row['label']:\n",
    "            correct_labels+=1\n",
    "    accuracy=correct_labels/test_df.shape[0]\n",
    "    return predictions, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is:  0.2082294264339152\n"
     ]
    }
   ],
   "source": [
    "_, acc=classifier_1_evaluate(class_words_prob)\n",
    "print('The accuracy of the model is: ',acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram, Bigram, and n-gram\n",
    "\n",
    "In the previous classifier, we used unigrams, in which we consider the single words independently. But different words may have different meanings together implicitly. Hence, using bigrams and even n-grams helps us better understand the meaning of the expressions in the sentence. For example:\n",
    "- Why is this so hard to use?\n",
    "- This glass is hard enough to not be broken.\n",
    "\n",
    "The word \"hard\" is in two different sentences with different meanings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additive Smoothing\n",
    "\n",
    "For example, the word \"screen\" may be in both the technology and gaming categories. But suppose it is present in a gaming article in the training data and not in a technology one. As a result, the classifier assumes that this word belongs to a gaming class that is not valid in practice. So, as the word \"screen\" is not present in the technology class, its probability is zero, in this case, minus infinity because of the logarithm. Therefore, other words that may have a good probability in the technology class be eliminated, and the sentence's probability would be zero for that particular class.\n",
    "- (Class Gaming in train data) The Game plays well on all screen resolutions.\n",
    "- (Class Technology in test data) The brand new mobile phone has a great touch screen.\n",
    "\n",
    "If a word did not exist in our training data, it would get the probability of log(0), which is minus infinity, and that class's likelihood of being the one would be almost zero. Therefore by applying the additive smoothing, we ensure that the probability never will be minus infinity. So, for the above particular sentence, the likelihood of classifying the second sentence as technology would be considerable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics, additive smoothing, or Lidstone smoothing, is a technique used to smooth categorical data.\n",
    "Given an observation x = (x1, …, xd) from a multinomial distribution with N trials and parameter vector θ = (θ1, …, θd), a \"smoothed\" version of the data gives the estimator:\n",
    "\n",
    "![Additive Smoothing](./img/AdditiveSmoothing.png)\n",
    "\n",
    "where the pseudo count α > 0 is the smoothing parameter (α = 0 corresponds to no smoothing), additive smoothing is a shrinkage estimator, as the resulting estimate will be between the empirical estimate xi / N and the uniform probability 1/d. Using Laplace's rule of succession, some authors have argued that α should be 1 (in which case the term add-one smoothing is also used), though, in practice, a smaller value is typically chosen ([Source](https://medium.com/syncedreview/applying-multinomial-naive-bayes-to-nlp-problems-a-practical-explanation-4f5271768ebf)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
